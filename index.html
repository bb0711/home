<!DOCTYPE html>
<link rel="stylesheet" href="style.css">
<html>
<head>
<title> Heeju Wi's homepage</title>
</head>
<body width = 900px leftmargin="250" topmargin="70" rightmargin="250" alink="#004680" vlink="#004680" link="#004680" text-align:center>
<div class = outer>
<div class = head_div>
<div class="lw">wi hee ju</div>
<div class="lv1">data scientist, developer</div>
  <div class="menu">&#x1F606; About Me </div>
  <a href ="#project_part"><div class="menu">&#x1F4BB; Projects </div></a>
  <a href="#email_part"><div class="menu">&#x1F48C; Contact </div></a>
</div>
</div>
  <div class="box">
  <div class="lv2">
    About Me
  </div>
    <div class="lv3">
      <p>
        I want to help people to live better life by making the better products which people really wants. <br>
        I believe Data Science can do that so I am trying to learn it.<br>
        I'm interested in application of ML/AI with real data for actual usage.<br>
        Especially, I was interested in <b>recommendation system, <a href='files/Fairness_summary.pdf' target="_blank">fairness of ML system</a>, <a href='https://docs.google.com/presentation/d/1o3KC46dFLkVUrBGWNdDe2QrS0YofELQ0jj1wOvcDwsA/edit?usp=sharing' target="_blank">online bot</a> and hate speech.</b><br>
        Now I'm working on <b>Building Korean Readability Index using AI.</b>
      </p>

    </div>
  
  <div class="lv2" id="project_part">
    Research & Publications
  </div>
    <div class="lv3">
      <div class = p_box>
        <div class= paper_title> Developing new Korean Readability Index using AI.
        </div>
        <div class= paper_academy>Now working on it !</div>
        <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> Korean NLP, Readability Index, Big Data </div>
        <div class = "paper_contents korean"> <b>summary&nbsp;</b>가독성 지표(Readability Index)는 글을 이해할 수 있는 독자의 최소 나이를 의미한다. 
          이를 활용하여 텍스트 분석을 통해 작성자의 수준 및 의도를 파악할 수 있으며, 책 추천 및 작문 평가 등을 할 수 있어 교육 분야에도 유용한 지표로 활용할 수 있다. 
          하지만 영어에 비해 한국어로 연구가 거의 진행되지 않아, 최신 기술의 AI를 활용해 한글 가독성 지표를 제작하고자 한다.
          사용할 데이터는 전문가들이 제작한 국어 교과서 텍스트 데이터이며 해당 텍스트의 학년을 예측하는 모델을 제작하고자 한다.
          KLUE 모델의 word embedding 와 KLUE-Dependency Parser를 이용해 텍스트를 어휘와 문법 구조 각각의 의미를 담고 있는 값으로 변환한 후, attention mechanism과 RNN을 적절히 결합한 모델을 사용하고자 한다.
          현재는 어떤 모델이 더 적합할지 탐색해보는 단계에 있다.
        </div>
      </div>


      <div class = p_box>
        <div class= paper_title>A Two-staged Neural Network Model for Associating Data with Multiple Users in A Smart Environment
          <div class= "attaches pdf">
            <a href="files/hci2020_whj.pdf">paper</a>
          </div>

        </div>
        <div class= paper_writer>Heeju Wi, Hyunju Kim and Dongman Lee</div>
        <div class= paper_academy>HCI Korea 2020</div>
        <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> Deep Learning, Smart Environment, Activity Recognition, Data Association </div>
        <div class = "paper_contents korean"> <b>summary&nbsp;</b>여러 IOT 기구들이 있는 스마트 홈에서는 센서를 이용해 거주자의 현재 상황을 파악하고 다음 행동을 예측하는 행위 인지 연구 (Activity Recognition)가 활발히 진행되고 있다. 
        2인 이상 거주하는 가구가 대다수이므로, 행위 인지를 실생활에 적용하기 위해서는 각 센서가 어떤 거주자로부터 발생하였는지 파악하는 '데이터-사용자 연관 (Data Association)' 연구가 반드시 필요하다.
        따라서 본 연구에서는 결합된 라벨링 기법 (Combined Labeling)과 LSTM을 활용한 TSM-LSTM 모델을 제안한다. CASAS 데이터셋을 이용하여 평가한 결과, 기존 연구의 모델보다 평균 정확도가 76%에서 79%로 증가하였으며, 예측값의 범위 또한 감소하였다.
      </br>
        </div>
      </div>


      <div class = p_box>
        <div class= paper_title>Personalizing the Prediction: Interactive and Interpretable machine learning
          <div class= "attaches pdf">
            <a href="files/Final_Personalizing the prediction.pdf">paper</a>
          </div>

        </div>
        <div class= paper_writer>H. J. Wi, S. Koh, B. Hyung Kim and S. Jo</div>
        <div class= paper_academy>2019 16th International Conference on Ubiquitous Robots (UR), 2019.</div>
        <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> XAI(Explainable AI), Recommendation System, Big Data, HCI, User Feedback </div>
        <div class = paper_contents style="display: none;">Proposing the new algorithm of the feedback-able movie recommendation system using XAI (Explainable AI). It can get detailed feedback which was never existed in Youtube or other movie recoomendation platform at that time. For the results, we did user servey and the test subjects said that this system gives better credibility for the users.</br>
        
        </div>
        <div class = "paper_contents korean"> <b>summary&nbsp;</b>연구를 진행할 당시 Youtube와 다른 동영상 추천 플랫폼들이 추천 이유를 제공하지 않는 것에 불편함을 느끼고 유저가 추천시스템에 다양한 피드백을 주고자 시작한 연구이다.
        X-MoRe (Explainable Movie Recommendation System) 라는 새로운 시스템을 구현하였으며, flask와 MongoDB를 통해 실제 사용할 수 있도록 GUI도 고려한 웹사이트까지 구현하였고 이를 이용해 유저 스터디 또한 진행하였다. MovieLens 데이터셋과 IMDB 사이트를 크롤링한 데이터 일부를 결합하여 이용하였다.
        추천시스템은 영화별 정보와 tag의 word embedding을 이용하여 content based filtering 방법으로 구현하였으며, clutering으로 크게 두 그룹으로 나누고 이후 regression을 거쳐 영화별 별점을 예측하는 모델이다.
        X-MoRe은 XAI 모델 중 하나인 LIME을 이용하여 추천된 영화당 배우, 감독, 태그, 장르, 개봉 시기의 5개 카테고리 항목에 대해 가장 강력한 6개의 추천 이유와 그 영향력을 보여준다. 유저는 이 추천 이유를 드래그를 통해 조절함으로써 추천 시스템에 피드백을 줄 수 있으며, 이는 실시간으로 추천 시스템에 반영된다.
        총 23명의 유저 스터디를 진행하여 평가한 결과, 다수가 X-MoRe가 보다 정확한 추천 시스템을 제공하며, 편리하게 개인화된 추천이 가능하도록하여 시스템에 대한 신뢰성이 증가한다고 답하였다.
        </div>
      </div>
    </div>


  <div class="lv2" id="project_part">
    Projects
  </div>
    <div class="lv3">

      <div class = p_box>
        <div class= paper_title>How Can We Detect Toxicity for Korean?: Toxic Comments Classification for Korean Movie Comments 
          <div class= "attaches ppt">
          <a href="files/hatespeech_ppt.pdf">ppt</a>
        </div>
        <div class= "attaches poster">
          <a href="https://github.com/bb0711/korean_toxic_classifier">git</a>
        </div></div>
            Implementing Korean toxic comments classifier using CNN. For the labeling, we used crowdsourcing by our small web with Naver movie review comments.
            
            <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> Hate speech, Big Data, Crowdsourcing, Deep Learning, NLP</div>
            <div class = "paper_contents korean"> <b>summary&nbsp;</b> Google의 toxicity 판별기인 Perspective API가 한글은 지원하지 않고, (기존 연구에 따르면) 한글 악성 댓글과 영어 악성 댓글의 특징이 다르다는 점에서 시작한 연구이다.
              네이버 영화 평가 데이터를 크롤링해서 사용하였으며, 데이터 라벨링은 crowdsourcing 방식으로 직접 firebase와 flask를 이용해 크라우드소싱-웹사이트를 구축하여 twitter에 배포하여 진행했다.
              tokenizing은 음절 수준과 자음모음 수준의 두가지 방식으로 진행하여 비교하였다. word embedding으로는 Fasttext를, 예측 모델로는 pytorch를 이용해 구현한 textCNN 모델을 이용하였다.
              음절 수준의 tokenizing이 더 좋은 결과를 보였으며, 악성댓글과 그렇지 않은 댓글이 1:1 비율로 있는 test data에 대해 평가하였을 때, 81.3%의 정확도를 보였다.
              또한 이 발표자료에서는 해당 모델의 개선해야할 점뿐만 아니라 한글 악성 댓글 판별기가 필요한 이유와 실제 적용가능한 사용법 그리고 이 모델 혹은 AI를 활용한 모델의 문제점 또한 제시한다.
              </div>
              <div class= paper_academy>Mar. 2019 ~ Jul. 2019</div>


      </div>

      <div class = p_box>
        <div class= paper_title>Reducing Bias and Guarantee of Fairness on Recommendation System Focusing on Measurement Methodology

          <div class= "attaches ppt">
            <a href="files/(2020-11-20)추천시스템편향보정연구.pdf">report</a>
          </div>
        </div>
        <div class = "paper_contents korean">KISDI (정보통신정책연구원)와 건국대학교 산학협력단과 함께 진행한 '추천시스템의 편향 보정 및 공정성 보장 방안 연구 : 측정방법론을 중심으로' 연구.</div>
            
            <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> Fairness in Recommnedation System,  Reducing Bias Disparity</div>
            <div class = "paper_contents korean"> <b>summary&nbsp;</b> 추천 시스템에 요구되는 편향의 보정과 공정성 보장을 개념적, 실증적으로 고찰하고 알고리즘 편향을 보정하기 위한 pre-processing 방식의 4가지 데이터 변형 방식을 제안한다.
              실제 플랫폼에서 사용하는 추천 알고리즘을 알 수 없기 때문에, 평가를 위해 User-KNN과 SVD알고리즘 각각을 사용하여 간단한 추천 시스템을 구현하였으며 가상으로 생성한 유저 행동기록 데이터와 실제 MovieLens 데이터를 이용하여 평가하였다. 
              실험 결과 추천시스템을 통해 추천된 항목들이 기존 유저의 선호도보다 더 증폭된 정도로 추천된 항목이라는 것을 확인하였으며, 이를 통해 알고리즘으로 인해 편향이 발생할 수 있음을 확인했다. 
              제안하는 편향성 보정 방법을 적용하면, 알고리즘 자체를 수정하지는 않기 때문에 적은 비용으로 보정을 할 수 있지만 모든 방식이 모든 알고리즘에 좋은 효과를 보이지는 않았다. 
              본인은 이 연구에서 기존 추천시스템의 공정성 관련 선행 연구 조사, 편향성 보정 방법 제안 및 구현, 추천시스템 실험 설계 및 구현, 결과 분석 및 제안 등을 맡았다.
              </div>
              <div class= paper_academy>Jun. 2020 ~ Nov. 2020</div>


      </div>

      <div class = p_box>
        <div class= paper_title>NMOEA: Node Embedding and Multi-Objective Evolutionary Algorithms for Co-authorship Prediction

          <div class= "attaches ppt">
              <a href="files/ai506_report.pdf">ppt</a>
            </div>
            <div class= "attaches poster">
              <a href="https://github.com/bb0711/NMOEA">git</a>
            </div>
        </div>
        Final Term Project for KAIST-AI506 (Data Mining and Search)
            
            <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> Muti-Objective Evolutionary Algorithm, Node Embedding, Co-authorship Prediction</div>
            <div class = "paper_contents korean"> <b>summary&nbsp;</b> community detection과 co-authorship problem에 node embedding은 자주 쓰이나 MOEA(Multi-Objective Evolutionary Algorithm)과 결합해서는 잘 쓰이지 않아 해당 방법을 사용하여 Co-authorship prediction task를 해결하고자 하였다. 
              node2vec을 이용하여 node간의 homophily와 structural equivalence가 각각 내재된 두 개의 feature를 추출할 수 있을 것이라고 가정하였다.
              MOEA는 목적하고자 하는 값이 두개 이상일 때 사용하는 알고리즘으로, 이 프로젝트에서는 local similarity 와 structural role dissimilarity의 2개의 fitness function을 정의하였다.
              이 모델은 정의한 NMOEA algorithm을 따라 pareto front partitions인 list of set of nodes을 구하고, 각각의 set of nodes가 co-author가 맞는지 아닌지 예측하는 SVM 모델을 추가로 학습하여 최종 co-author를 예측한다.
              팀원 한명과 함께 한 팀 프로젝트로, 전반적인 알고리즘 설계, NMOEA 알고리즘 구현, 리포트 작성을 맡았다.
              </div>
              <div class= paper_academy>Mar. 2020 ~ Jul. 2020</div>


      </div>


      <div class = p_box>
        <div class= paper_title>Analysis of Daejeon's Dangerous Area
          <div class= "attaches ppt">
            <a href="files/daejeon_traffic_ppt.pdf">ppt</a>
          </div>
        </div>
        <div class = "paper_contents korean">[공모전] LH 한국토지주택공사에서 주관한 공모전, 3등상 수상.
          
        </div>
            
            <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> Data Analysis, Big Data, Traffic Data</div>
            <div class = "paper_contents korean"> <b>summary&nbsp;</b> 대전시의 실제 교통 및 토지 데이터를 다룰 수 있는 좋은 기회라고 생각하여 해당 공모전에 참여하였다. 과제에서 위험지역의 정의를 주지 않아, 이 정의를 어떻게 내리는 지가 중요하였으며,
              본인은 어떤 사고 횟수 예측모델이 있다고 가정할 때, 예측한 사고 횟수에 비해 실제 사고 횟수가 현저히 적으면 위험한 지역이라고 정의하였다. 
              이 사고 횟수 예측 모델로는 국내외의 기존 교통사고 위험 연구 조사를 바탕으로 성능이 좋은 딥러닝 모델을 채택하였고, XAI 모델인 LIME을 이용하여 그 추론 원인도 파악하고자 하였다.
              EDA를 통해 대전시 교통사고 추이 분석, 피해자 연령대별 사고 유형 분석, 도로의 혼잡빈도와 사고 관계 분석, 교통 안전물과 사고 유형 간의 관계 분석을 진행하였다.
              여러 feature들 중 상관 계수 분석을 통한 그룹화, 변수 생성, 제거 등을 통해 총 18 개의 feature를 채택하였고, pytorch를 이용해 4-layer DNN 모델을 제작하였다. 
              이 모델은 실제 사고가 발생한 지역에 대해 사고 지역으로 예측할 확률이 88%이며 사고가 많이 발생한 지역일수록 정확도가 높아졌다.
              이후 추출한 위험지역을 3가지 유형으로 군집화하여 분석하였고, LIME 결과물을 이용해 위험지역으로 뽑힌 원인을 분석하고 이를 interactive web으로 시각화하였다.
              또한 추가로 제안하는 위험지역의 교통안전물 추가 효과를 분석하였는데, 위험지역이 아닌 지역과는 달리 위험지역은 제안하는 교통안전물을 추가할 경우 3년간 평균 3.52건의 사고 감소 효과가 있다고 나타났다.
              조사한 바에 따르면 국내 데이터를 딥러닝 모델을 활용하여 넓은 범위의 지역의 교통사고를 예측한 최초의 연구이다.
              
              </div>
              <div class= paper_academy>Mar. 2021 ~ Jun. 2021</div>


      </div>

      <div class = p_box>
        <div class= paper_title>Implementation of KLUE Dependcy Parser API
          <div class= "attaches poster">
            <a href="https://github.com/bb0711/run_klue_dp">git</a>
          </div>
        </div>
            Implemented Dependency Parser for Korean using KLUE and made it easily for anyone to use.
            
            <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> Dependency Parser API</div>
            <div class = "paper_contents korean"> <b>summary&nbsp;</b> 최근 공개된 <a href='https://github.com/KLUE-benchmark/KLUE'>KLUE</a> (Korean Language Understanding Evaluation)에서 의존 문법 파서(Dependency Parser)를 위한 데이터셋과 모델을 공개하였다.
              코드 또한 공개되어 있으나, competition을 위해 코드를 공개하였기 때문에 실행을 위한 설명이 없으며, 두 코드를 합쳐서 모델을 학습 후 사용하여야 한다.
              모든 과정이 너무 복잡하여 단순히 한글 의존 문법 구조 분석을 위해 SOTA 모델을 사용하려는 연구자들은 사용을 할 수 없는 수준이었다.
              따라서 비교적 쉽게 모델을 사용할 수 있도록 두 코드를 병합하였으며, 일부 error를 수정하였고, 모델을 미리 학습시켰으며, 결과물을 쉽게 확인 할 수 있도록 변환하였다.
              또한 기존 KLUE-DP 모델의 경우 전문가가 직접 라벨링한 품사 태깅값을 추가 입력값으로 사용하였으므로, mecab-ko를 결합하여 형태소 분석을 진행 후 KLUE-DP 모델에 들어가도록 수정하였다.
              자세한 사용방법 및 mecab-ko를 이용한 형태소 분석 결과와 기존 모델의 성능 차이는 위의 github에 작성해 두었다.
              추후 더 간단히 python PyPI 로 배포할 예정에 있다.
            </div>

            <div class= paper_academy>Aug. 2021</div>
            
        </div>

      <div class = p_box>
        <div class= "paper_title korean">대한민국 유통 활성화를 위한 적요 표준화 
          <div class= "attaches ppt">
            <a href="files/수비드데이터_final.pdf">ppt</a>
          </div>
        </div>
            <div class = "paper_contents korean">더존비즈온 주최 2020 빅데이터 경진대회 2등상 수상 </div>
            
            <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> Data Mining, Social Data, Data-driven</div>
            <div class = "paper_contents korean"> <b>summary&nbsp;</b> 적요(물품 흐름을 파악할 수 있도록 회계 기록을 작성한 데이터) 데이터는 수기로 작성하는 경우가 많아, 작성자의 주관적 판단에 따라 작성되어 표준화가 어렵다는 문제점을 해결하기 위한 프로젝트였다. 
              따라서 물품 분류 목록을 만들어 기존 적요 데이터를 맵핑하도록 설계하였으며, 주어진 데이터로 자체 제작한 딕셔너리 사용, 외부 DB사용 그리고 google search와 word2vec을 사용하는 총 3단계의 맵핑 프로세스를 제안하였다.
              물품 분류 목록은 따로 주어지지 않아 조달청의 상품정보시스템에 몇 가지 항목을 추가로 더해 사용하였다.  
              데이터 가공은 konlpy를 이용한 명사 분해와 불용어 제거를 통해 진행되었으며, 이후 표지 단어를 추출하여 해당 표지 단어를 포함하는 경우 우선 분류되도록 하였다.
              이후 조달청 상품정보 시스템에 직접 검색해 나오는 품목은 해당 분류로 분류한다.
              직접 python으로 제작한 google 검색을 하여 첫 페이지의 모든 단어를 크롤링해 날짜/url/지명/조사를 제외한 최다 빈도 단어 두 개를 추출하였고, 이 단어와 물품 분류 항목 간의 word2vec을 이용해 cosine similarity가 높은 항목으로 라벨링하였다. 
              이는 빅데이터 기반으로 구현되어 주관적인 판단을 배제할 수 있는 장점이 있다.
              본인은 전반적인 알고리즘 설계 및 모든 알고리즘 구현을 맡았다.

            <div class= paper_academy>Jun. 2019 ~ Nov. 2019</div>
            
        </div>
      </div>

      <div class = p_box>
        <div class= paper_title>Analysis of External Environmental Factors Affecting Venture Business Stability through Social Data Analysis: Based on the Food Service Franchises
          <div class= "attaches ppt">
            <a href="files/2019_venture_ppt.pdf">ppt</a>
          </div>
          <div class= "attaches pdf">
            <a href="files/2019_venture.pdf">paper</a>
          </div>
        </div>
            Analyzing twitter text data to figure out the factors which are related to the sustatinable success of the food franchise companies.
            
            
            
            <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> Data Analysis, Social Data, Sequential Data, NLP</div>
            <div class = "paper_contents korean"> <b>summary&nbsp;</b> 과연 망하는 음식점과 그렇지 않은 음식점을 SNS 데이터를 보고 예측할 수 있을까? 에서 시작한 연구이다.
              분석 대상은 비교적 소셜 데이터를 얻기 쉬운 대형 외식 프랜차이즈 기업으로 제한하였으며, 창업한지 5년 이상이 지났으며 그 지점 수가 유지되거나 증가하는 기업을 안정성이 높은 기업, 창업 후 2-3년 이내로 적자 전환되어 지점 수가 줄어드는 기업을 안정성이 낮은 기업으로 정의하여 총 5개의 분석 기업을 선정했다.
              SNS는 twitter로 한정하였으며, 크롤링을 통해 각 기업별 상호명을 포함하는 트윗 데이터를 수집하였다. 데이터 전처리 이후, pretrained fasttext 모델을 이용해 단어 embedding을 구하고, cosine 유사도를 이용해 유사한 단어끼리 그룹화 하였다.
              연구는 두 가지 가설을 검증하고자 시간에 따른 작성 트윗 수 분석과 시간에 따른 키워드 흐름 분석의 두가지 방향으로 진행하였다. 예상과 달리 관련 트윗 개수 및 변화량은 기업의 안정성과 큰 관련이 없었다. 
              하지만 발생 빈도가 높은 키워드의 시간에 따른 경향성은 안정성과 높은 기업과 그렇지 않은 기업에 따라 차이가 있었다. 안정성이 높은 기업의 경우 키워드들의 증감 변동폭이 크지 않았으며, 특정 구간에 급증 후 급락한 일부 키워드 또한 긍정적 어휘이거나 특정 장소를 의미하는 단어였다. 
              하지만 안정성이 낮은 기업의 경우 우선 부정적인 단어들이 상위 빈도를 차지하며, 변동폭이 크고 변동이 주기적으로 발생한 키워드의 경우에 부정적인 어휘거나 홍보를 위해 사용된 전혀 의미없는 어휘였다.
              표본대상 수가 적고, 소셜 미디어를 한정하였다는 한계점이 있으나, 최초로 벤처기업의 성공 및 실패요인 분석에 소셜 빅데이터를 이용한 연구였으며, 새로운 객관적인 지표를 제안하였다는 쟁점이 있다.
            </div>

            <div class= paper_academy>Jun. 2019 ~ Nov. 2019</div>
            
        </div>

        
      
      <div class="lv2" id="project_part2">
        Below is projects that I did recently but not 100% in Data Science :)
      </div>

      <div class = p_box>
        <div class= paper_title>ALBAKA: Weekly Scheduling System for Part-timers
          <div class= "attaches ppt">
            <a href="https://youtu.be/GjgfWao66Ys">video</a>
          </div>
          <div class= "attaches poster">
            <a href="https://github.com/sooj-j/albaka">git</a>
          </div>
        </div>
            It is the term project of HCI(Human Computer Interaction) class in KAIST and we got 'Best Implementation Award' from the Prof. juho Kim.
          
            
            
            <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> HCI (Human Computer Interaction)</div>
            <div class = "paper_contents korean"> <b>summary&nbsp;</b> 유저를 위한 시스템에 대해 공부하는 HCI 수업의 term project로 진행하였다. 일부 대형 프랜차이즈 알바는 매주 시간표를 손으로 짜며, 알바생 간의 시간대 교환 및 대화 전달이 어렵다는 유저의 니즈에서 착안하여 시작하였다. 
              총 8번의 Design Project를 통해 개발되었으며, Ideation, Prototypying, User-testing 등을 진행하였다. Javascript, html 과 Firebase를 이용해 구현하였다. 
              front-end를 해본적 없는 4명이 모인 것 치곤 스스로 생각해도 매우 잘 만들었으며 실제로 교수님께서 한 팀에만 주시는 'Best Implementation Award'를 받았지만 현재 firebase 연결을 해지하여 체험해볼 수 없다 :( (다만 유튜브 링크로 확인이 가능하다) 
            </div>

            <div class= paper_academy>Mar. 2019 ~ Jul. 2019</div>
            
            
      </div>

      <div class = p_box>
        <div class= paper_title>Better Image De-occlusion using Image Search
          <div class= "attaches pdf">
            <a href="files/CS570_Deocclusion.pdf">paper</a>
          </div>
          <div class= "attaches ppt">
            <a href="files/cs570ppt.pdf">ppt</a>
          </div>
        </div>
            
            <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> Scene De-occlusion, Amodal Mask, Content Generation</div>
            <div class = "paper_contents korean"> <b>summary&nbsp;</b> 여러 물체가 있는 이미지가 있을 때, 일부 가려진 부분이 있더라도 각 물체별 이미지를 복귀하는 것을 Image De-occlusion 이라고 한다. 
              이 프로젝트는 'Self-Supervised Scene De-occlusion(CVPR 2020)'의 논문을 replicate하고 google image search를 결합하여 모델을 개선하고자 하였다. 
              논문의 모델은 가려진 mask를 복귀시키는 PCNet-M과 마스크 내부의 색을 채우는 PCNet-C로 구성된다.
              google Image search로 가져온 유사한 이미지들에서 SIFT (Scale-Invariant Feature Transform) feature를 추출하고 이중 knn 을 이용해 occluded image와 유사한 이미지 집단을 찾는다. 기존 PCNet-C에 reference image를 추가한 새로운 convolutional model을 구현했다.
              데이터는 기존 논문에서 사용한 COCOA dataset을 이용하여 평가하였다.
              본인은 이 프로젝트에서 논문 replicate 부분, 모델 평가, 전반적인 알고리즘 설계를 맡았다.
            </div>

            <div class= paper_academy>Mar. 2020 ~ Jul. 2020</div>
            
            
      </div>

      <div class = p_box>
        <div class= paper_title>MiniCS: Critical Section Minimisation in Concurrent Programming
          <div class= "attaches pdf">
            <a href="files/MiniCS_report.pdf">paper</a>
          </div>
          <div class= "attaches ppt">
            <a href="https://docs.google.com/presentation/d/1tDEl-Vi_QWYiuUN1uQit4kwQI2XChnlzreQjt5BmeJg/edit?usp=sharing">ppt</a>
          </div>
          <div class= "attaches poster">
            <a href="https://github.com/hyunsukimsokcho/miniCS">git</a>
          </div>
        </div>
            Using the genetic algorithm, we tried to implement the program which automatically prevents race conditions in concurrent programming.
            
            
            
            <div class = "paper_contents korean keywords"> <b>keywords&nbsp;</b> AI Based Software Engineering, Genetic Algorithm, Concurrent Programming</div>
            <div class = "paper_contents korean"> <b>summary&nbsp;</b> Multi-thread 프로그래밍의 경우, 2개 이상의 thread가 같은 data에 접근할 경우 발생하는 data race problem이 자주 발생한다. 
              이 문제를 해결하기 위해 Clang에서 추출한 AST format을 이용하여 (lock acquire 시기, lock release 시기) 형식으로 유전자를 정의하였고, 이후 Genetic Algorithm을 이용하여 문제를 해결하고자 하였다. 
              이는 Population generation, muation, crossover 그리고 fitness evaluation 순서대로 진행되며, 우리가 제안하는 모델의 경우 data race free code로 수정하거나 기존 코드에 비해 lock interval을 2/3로 줄이는 효과를 보였다. 
              이 프로젝트는 CS454 (AI Based Software Engineering)의 final 프로젝트로 4명의 팀원과 함께 진행하였으며, 본인은 전체적인 연구 설계, GA 식 설계, 테스트 케이스 제작 및 평가 구현을 맡았다. 
            </div>

            <div class= paper_academy>Sep. 2019 ~ Dec. 2019</div>
            
            
      </div>
    </div>
    
    </div>

  <div class="lv2">
    Education
  </div>
    <div class="lv3">
      M.S. in <a href="https://cs.kaist.ac.kr">School of Computing</a>, <a href="https://www.kaist.ac.kr/html/en/index.html">KAIST</a>. (Mar. 2020 ~ present)<br>
      B.S. in <a href="https://cs.kaist.ac.kr">School of Computing</a>, <a href="https://www.kaist.ac.kr/html/en/index.html">KAIST</a>. (Mar. 2015 ~ Feb. 2020)
    </div>
    
  <div class="lv2">
    Experiences
  </div>
  <div class="lv3">
    <b>Teaching Assistant</b> in CS101 (Introduction to Programming), CS206 (Data Structure) and CS564 (Big Data Analytics Using R) of KAIST. (for 4 semeters, Mar. 2020 ~ Feb. 2022)<br/>
    <b>Research Intern</b> in <a href="http://cds.kaist.ac.kr/cdsn/"  >CDSN Lab</a>, KAIST. (Sep. 2019 ~ Dec. 2019)<br/>
    - Researched on smart home environment especially on user-data association problem.<br/>
    <b>Data Scientist Intern </b> in <a href="http://company.netmarble.com">Netmarble</a>, Seoul. (Jun. 2019 ~ Aug. 2019)<br>
    - Proposed better user-matching algorithm which reduces bounce rate (이탈률) with 모두의 마블's raw data using SQL and python.<br>
    <b>ML Engineer Intern</b> in CUOP, Seoul. (Jan. 2018 ~ Apr. 2018)<br>
    - Implemented backend ML service that predicts student's coding ability.<br>
    - Implemented frontend to get the users' action data from the coding learning website using React, JS, MongoDB and python.<br>
  </div>
   <div class="lv2" id="email_part">
    Contact<br>
  </div>
    <div class="lv3">
      email: bb0711@kaist.ac.kr<br/>
    </div>
  
</body>
</html>